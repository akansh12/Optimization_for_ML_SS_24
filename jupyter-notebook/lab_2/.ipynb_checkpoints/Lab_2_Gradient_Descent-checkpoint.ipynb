{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cR9Mnv7rFbL"
      },
      "outputs": [],
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kV6gpOAFrFbM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are running Python 3. Good job :)\n"
          ]
        }
      ],
      "source": [
        "# Check the Python version\n",
        "import sys\n",
        "if sys.version.startswith(\"3.\"):\n",
        "  print(\"You are running Python 3. Good job :)\")\n",
        "else:\n",
        "  print(\"This notebook requires Python 3.\\nIf you are using Google Colab, go to Runtime > Change runtime type and choose Python 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5UIQRi7KrFbM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "  # Clone the entire repo to access the files.\n",
        "  !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
        "  %cd cloned-repo/labs/ex02/template/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jFK_K-ZrFbM"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vZdSHuAyrFbN"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from helpers import *\n",
        "\n",
        "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
        "x, mean_x, std_x = standardize(height)\n",
        "b, A = build_model_data(x, weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ntJDnBajrFbN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples n =  10000\n",
            "Dimension of each sample d =  2\n"
          ]
        }
      ],
      "source": [
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xq0jdnsrFbN"
      },
      "source": [
        "# Least Squares Estimation\n",
        "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
        "\n",
        "In this exercise, we will try to fit $x$ using Least Squares Estimation.\n",
        "\n",
        "One can see the function is $L$ smooth with $L =\\frac1n\\|A^T A\\|  = \\frac1n\\|A\\|^2$ (Lemma 2.3 for the first equality, and a few manipulations for the second)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-YmQ_0ArFbO"
      },
      "source": [
        "# Computing the Objective Function\n",
        "Fill in the `calculate_objective` function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GyI5YMcYrFbP"
      },
      "outputs": [],
      "source": [
        "def calculate_objective(Axmb):\n",
        "    \"\"\"Calculate the mean squared error for vector Axmb = Ax - b.\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    return 1/2*np.mean(Axmb**2)\n",
        "    \n",
        "    # TODO: compute mean squared error\n",
        "    # ***************************************************\n",
        "    # raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDt-eCxwrFbP"
      },
      "source": [
        "# Compute smoothness constant $L$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw1mqbIsrFbP"
      },
      "source": [
        "To compute the spectral norm of A you can use np.linalg.norm(A, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "skZB8hqwrFbQ"
      },
      "outputs": [],
      "source": [
        "def calculate_L(b, A):\n",
        "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute ||A.T*A||\n",
        "    L = np.linalg.norm(A.T @ A, 2)\n",
        "    L = L / len(b)\n",
        "    # ***************************************************\n",
        "    # raise NotImplementedError\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute L = smoothness constant of f\n",
        "    # ***************************************************\n",
        "    # raise NotImplementedError\n",
        "    return L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dN2ncYKrFbQ"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dALAjP5CrFbQ"
      },
      "source": [
        "Please fill in the functions `compute_gradient` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WpwoJZIRrFbQ"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(b, A, x):\n",
        "    \"\"\"Compute the gradient.\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute gradient and objective\n",
        "    Axmb = A @ x - b\n",
        "    grad = A.T @ Axmb\n",
        "    grad = grad / len(b)\n",
        "    \n",
        "    # ***************************************************\n",
        "    # raise NotImplementedError\n",
        "    return grad, Axmb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiKGUH0lrFbQ"
      },
      "source": [
        "Please fill in the functions `gradient_descent` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CNVn89bxrFbQ"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(b, A, initial_x, max_iters, gamma):\n",
        "    \"\"\"Gradient descent algorithm.\"\"\"\n",
        "    # Define parameters to store x and objective func. values\n",
        "    xs = [initial_x]\n",
        "    objectives = []\n",
        "    x = initial_x\n",
        "    for n_iter in range(max_iters):\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: compute gradient and objective function\n",
        "        # ***************************************************\n",
        "        # raise NotImplementedError\n",
        "        grad, obj = compute_gradient(b, A, x)\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: update x by a gradient descent step\n",
        "        # ***************************************************\n",
        "        # raise NotImplementedError\n",
        "        x = x - gamma * grad\n",
        "        # store x and objective function value\n",
        "        xs.append(x)\n",
        "        objectives.append(obj)\n",
        "        print(\"Gradient Descent({bi}/{ti}): objective={l}\".format(\n",
        "              bi=n_iter, ti=max_iters - 1, l=obj))\n",
        "\n",
        "    return objectives, xs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMZ5EOJ-rFbR"
      },
      "source": [
        "Test your gradient descent function with a naive step size through gradient descent demo shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8BN0iFBvrFbR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Descent(0/49): objective=[-109.81967768  -73.68895452  -96.58434842 ...  -58.32779473  -74.38901745\n",
            "  -51.59669261]\n",
            "Gradient Descent(1/49): objective=[-99.8697465  -65.51366093 -86.54224031 ... -51.87416316 -66.12531311\n",
            " -45.81707416]\n",
            "Gradient Descent(2/49): objective=[-90.91480843 -58.15589669 -77.50434301 ... -46.06589474 -58.6879792\n",
            " -40.61541754]\n",
            "Gradient Descent(3/49): objective=[-82.85536416 -51.53390888 -69.37023545 ... -40.83845317 -51.99437868\n",
            " -35.93392659]\n",
            "Gradient Descent(4/49): objective=[-75.60186433 -45.57411984 -62.04953863 ... -36.13375575 -45.97013821\n",
            " -31.72058474]\n",
            "Gradient Descent(5/49): objective=[-69.07371447 -40.21030972 -55.4609115  ... -31.89952808 -40.54832179\n",
            " -27.92857707]\n",
            "Gradient Descent(6/49): objective=[-63.19837961 -35.3828806  -49.53114708 ... -28.08872317 -35.66868701\n",
            " -24.51577016]\n",
            "Gradient Descent(7/49): objective=[-57.91057822 -31.03819439 -44.19435911 ... -24.65899876 -31.27701571\n",
            " -21.44424395]\n",
            "Gradient Descent(8/49): objective=[-53.15155698 -27.12797681 -39.39124993 ... -21.57224678 -27.32451154\n",
            " -18.67987036]\n",
            "Gradient Descent(9/49): objective=[-48.86843786 -23.60878098 -35.06845167 ... -18.79417001 -23.76725778\n",
            " -16.19193412]\n",
            "Gradient Descent(10/49): objective=[-45.01363066 -20.44150474 -31.17793324 ... -16.29390091 -20.56572941\n",
            " -13.95279151]\n",
            "Gradient Descent(11/49): objective=[-41.54430417 -17.59095612 -27.67646664 ... -14.04365872 -17.68435387\n",
            " -11.93756317]\n",
            "Gradient Descent(12/49): objective=[-38.42191033 -15.02546237 -24.52514671 ... -12.01844075 -15.09111588\n",
            " -10.12385765]\n",
            "Gradient Descent(13/49): objective=[-35.61175588 -12.71651798 -21.68895877 ... -10.19574457 -12.75720169\n",
            "  -8.49152269]\n",
            "Gradient Descent(14/49): objective=[-33.08261687 -10.63846804 -19.13638963 ...  -8.55531802 -10.65667892\n",
            "  -7.02242122]\n",
            "Gradient Descent(15/49): objective=[-30.80639176  -8.76822309 -16.8390774  ...  -7.07893412  -8.76620843\n",
            "  -5.7002299 ]\n",
            "Gradient Descent(16/49): objective=[-28.75778917  -7.08500264 -14.77149639 ...  -5.75018861  -7.06478499\n",
            "  -4.51025772]\n",
            "Gradient Descent(17/49): objective=[-26.91404683  -5.57010423 -12.91067348 ...  -4.55431765  -5.53350389\n",
            "  -3.43928275]\n",
            "Gradient Descent(18/49): objective=[-25.25467873  -4.20669566 -11.23593287 ...  -3.47803378  -4.1553509\n",
            "  -2.47540528]\n",
            "Gradient Descent(19/49): objective=[-23.76124743  -2.97962795  -9.72866631 ...  -2.50937831  -2.91501321\n",
            "  -1.60791555]\n",
            "Gradient Descent(20/49): objective=[-22.41715927  -1.87526701  -8.37212642 ...  -1.63758838  -1.79870929\n",
            "  -0.8271748 ]\n",
            "Gradient Descent(21/49): objective=[-21.20747992  -0.88134217  -7.15124051 ...  -0.85297744  -0.79403576\n",
            "  -0.12450812]\n",
            "Gradient Descent(22/49): objective=[-2.01187685e+01  1.31901948e-02 -6.05244319e+00 ... -1.46827599e-01\n",
            "  1.10170415e-01  5.07891886e-01]\n",
            "Gradient Descent(23/49): objective=[-19.13892824   0.81826932  -5.0635256  ...   0.48870726   0.92395597\n",
            "   1.07705189]\n",
            "Gradient Descent(24/49): objective=[-18.257072     1.54284053  -4.17349977 ...   1.06068863   1.65636298\n",
            "   1.5892959 ]\n",
            "Gradient Descent(25/49): objective=[-17.46340138   2.19495462  -3.37247653 ...   1.57547187   2.31552928\n",
            "   2.05031551]\n",
            "Gradient Descent(26/49): objective=[-16.74909782   2.78185731  -2.65155561 ...   2.03877678   2.90877895\n",
            "   2.46523316]\n",
            "Gradient Descent(27/49): objective=[-16.10622462   3.31006972  -2.00272678 ...   2.4557512    3.44270365\n",
            "   2.83865904]\n",
            "Gradient Descent(28/49): objective=[-15.52763873   3.78546089  -1.41878083 ...   2.83102818   3.92323589\n",
            "   3.17474233]\n",
            "Gradient Descent(29/49): objective=[-15.00691144   4.21331295  -0.89322948 ...   3.16877746   4.3557149\n",
            "   3.47721729]\n",
            "Gradient Descent(30/49): objective=[-14.53825688   4.5983798   -0.42023327 ...   3.47275181   4.74494601\n",
            "   3.74944476]\n",
            "Gradient Descent(31/49): objective=[-1.41164678e+01  4.94493996e+00  5.46332634e-03 ...  3.74632873e+00\n",
            "  5.09525401e+00  3.99444948e+00]\n",
            "Gradient Descent(32/49): objective=[-13.73685757   5.25684411   0.38859026 ...   3.99254796   5.41053121\n",
            "   4.21495373]\n",
            "Gradient Descent(33/49): objective=[-13.39520839   5.53755784   0.7334045  ...   4.21414526   5.69428069\n",
            "   4.41340756]\n",
            "Gradient Descent(34/49): objective=[-13.08772413   5.7902002    1.04373732 ...   4.41358283   5.94965522\n",
            "   4.592016  ]\n",
            "Gradient Descent(35/49): objective=[-12.8109883    6.01757833   1.32303686 ...   4.59307665   6.1794923\n",
            "   4.75276359]\n",
            "Gradient Descent(36/49): objective=[-12.56192605   6.22221864   1.57440644 ...   4.75462108   6.38634567\n",
            "   4.89743643]\n",
            "Gradient Descent(37/49): objective=[-12.33777002   6.40639492   1.80063906 ...   4.90001107   6.5725137\n",
            "   5.02764199]\n",
            "Gradient Descent(38/49): objective=[-12.1360296    6.57215357   2.00424842 ...   5.03086206   6.74006493\n",
            "   5.14482698]\n",
            "Gradient Descent(39/49): objective=[-11.95446322   6.72133636   2.18749685 ...   5.14862795   6.89086104\n",
            "   5.25029348]\n",
            "Gradient Descent(40/49): objective=[-11.79105348   6.85560087   2.35242043 ...   5.25461726   7.02657753\n",
            "   5.34521333]\n",
            "Gradient Descent(41/49): objective=[-11.64398471   6.97643893   2.50085165 ...   5.35000763   7.14872238\n",
            "   5.4306412 ]\n",
            "Gradient Descent(42/49): objective=[-11.51162282   7.08519318   2.63443976 ...   5.43585897   7.25865274\n",
            "   5.50752627]\n",
            "Gradient Descent(43/49): objective=[-11.39249712   7.18307201   2.75466905 ...   5.51312517   7.35759007\n",
            "   5.57672284]\n",
            "Gradient Descent(44/49): objective=[-11.28528399   7.27116295   2.86287541 ...   5.58266475   7.44663366\n",
            "   5.63899976]\n",
            "Gradient Descent(45/49): objective=[-11.18879217   7.3504448    2.96026114 ...   5.64525037   7.52677289\n",
            "   5.69504898]\n",
            "Gradient Descent(46/49): objective=[-11.10194953   7.42179846   3.04790829 ...   5.70157743   7.5988982\n",
            "   5.74549328]\n",
            "Gradient Descent(47/49): objective=[-11.02379116   7.48601676   3.12679073 ...   5.75227179   7.66381098\n",
            "   5.79089315]\n",
            "Gradient Descent(48/49): objective=[-10.95344862   7.54381323   3.19778492 ...   5.79789671   7.72223248\n",
            "   5.83175303]\n",
            "Gradient Descent(49/49): objective=[-10.89014034   7.59583005   3.2616797  ...   5.83895914   7.77481184\n",
            "   5.86852692]\n",
            "Gradient Descent: execution time=0.052 seconds\n"
          ]
        }
      ],
      "source": [
        "# from gradient_descent import *\n",
        "from plots import gradient_descent_visualization\n",
        "\n",
        "# Define the parameters of the algorithm.\n",
        "max_iters = 50\n",
        "\n",
        "gamma = 0.1\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives_naive, gradient_xs_naive = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95-QVkXrFbR"
      },
      "source": [
        "Time Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DJcCvrzJrFbR"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntSlider, interact\n",
        "from grid_search import *\n",
        "\n",
        "def plot_figure(n_iter):\n",
        "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
        "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
        "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
        "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
        "\n",
        "    fig = gradient_descent_visualization(\n",
        "        gradient_objectives_naive, gradient_xs_naive, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
        "    fig.set_size_inches(10.0, 6.0)\n",
        "\n",
        "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs_naive)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeWW_ZFgrFbR"
      },
      "source": [
        "Try doing gradient descent with a better learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ53YQKLrFbR"
      },
      "outputs": [],
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = 50\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: a better learning rate using the smoothness of f\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ou0ebhmrFbR"
      },
      "source": [
        "Time visualization with a better learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w6-xTzVrFbR"
      },
      "outputs": [],
      "source": [
        "def plot_figure(n_iter):\n",
        "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
        "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
        "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
        "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
        "\n",
        "    fig = gradient_descent_visualization(\n",
        "        gradient_objectives, gradient_xs, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
        "    fig.set_size_inches(10.0, 6.0)\n",
        "\n",
        "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x85e_SXXrFbR"
      },
      "source": [
        "# Loading more complex data\n",
        "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPvXU3kprFbR"
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"Concrete_Data.csv\",delimiter=\",\")\n",
        "\n",
        "A = data[:,:-1]\n",
        "b = data[:,-1]\n",
        "A, mean_A, std_A = standardize(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbIxfzckrFbR"
      },
      "outputs": [],
      "source": [
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0u7FVCDrFbR"
      },
      "source": [
        "# Running gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7geA_ZWdrFbR"
      },
      "source": [
        "## Assuming bounded gradients\n",
        "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). Then by $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aErgE3rfrFbS"
      },
      "outputs": [],
      "source": [
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: Compute the bound on the gradient norm\n",
        "# ***************************************************\n",
        "grad_norm_bound =\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvoQjgcrrFbS"
      },
      "source": [
        "Fill in the learning rate assuming bounded gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci6syYe3rFbU"
      },
      "outputs": [],
      "source": [
        "max_iters = 50\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: Compute learning rate based on bounded gradient\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "bd_gradient_objectives, bd_gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
        "\n",
        "# Averaging the iterates as is the case for bounded gradients case\n",
        "bd_gradient_objectives_averaged = []\n",
        "for i in range(len(bd_gradient_xs)):\n",
        "    if i > 0:\n",
        "        bd_gradient_xs[i] = (i * bd_gradient_xs[i-1] + bd_gradient_xs[i])/(i + 1)\n",
        "    grad, err = compute_gradient(b, A, bd_gradient_xs[i])\n",
        "    obj = calculate_objective(err)\n",
        "    bd_gradient_objectives_averaged.append(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcodYaQOrFbU"
      },
      "source": [
        "## Gradient descent using smoothness\n",
        "Fill in the learning rate using smoothness of the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEdQ-J4JrFbV"
      },
      "outputs": [],
      "source": [
        "max_iters = 50\n",
        "\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: a better learning rate using the smoothness of f\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOHHcBKWrFbV"
      },
      "source": [
        "## Plotting the Evolution of the Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oQUjb3UrFbV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.xlabel('Number of steps')\n",
        "plt.ylabel('Objective Function')\n",
        "#plt.yscale(\"log\")\n",
        "plt.plot(range(len(gradient_objectives)), gradient_objectives,'r', label='gradient descent with 1/L stepsize')\n",
        "plt.plot(range(len(bd_gradient_objectives)), bd_gradient_objectives,'b', label='gradient descent assuming bounded gradients')\n",
        "plt.plot(range(len(bd_gradient_objectives_averaged)), bd_gradient_objectives_averaged,'g', label='gradient descent assuming bounded gradients with averaged iterates')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
